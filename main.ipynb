{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3470ab0a",
   "metadata": {},
   "source": [
    "Finally, we can do some useful work when mining crypto! The paper [Proofs of Useful Work from Arbitrary Matrix Multiplication](https://arxiv.org/abs/2504.09971) shows that instead of mining Bitcoin by checking hashes, a fundamentally useless operation, we can instead use matrix matrix multiplication, a very useful operation, to mine for BitCoin. This means that when we run AI algorithms, which have tons of matrix multiplications, we can mine Bitcoin for free! Super cool.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/paper.png\" alt=\"logo\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d5be6",
   "metadata": {},
   "source": [
    "The main algorithm has two core components: `solve` and `verify`. The solver can run `solve` and tries to convince the verifier that they did a moderate amount of work. The verifier runs `verify` to check that the solver actually did moderate work. We want to find a task that is difficult enough that it requires moderate work but simple enough that it is easy to check the solution. Running `solve` should be computationally expensive but running `verify` should be computationally cheap. Crucially, if there is a malicious solver who does not actually want to do this computationally expensive work, they w.h.p will not be able to fool the verifier.\n",
    "\n",
    "In traditional bitcoin PoW, the task is to find a number whose hash has $s$ leading zeros. This is a useless computation. `solve` does a brute force search over all strings, hashes each one, and returns any strings whose hash has $s$ leading zeros. `verify` simply takes the string returned by the solver and runs it through the hash function to confirm that indeed it has $s$ leading zeros. `solve` is computationally expensive because it does brute force search but `verify` is computationally cheap because it just runs a hash function once.\n",
    "\n",
    "In this paper's PoUW, the task is to multiply two matrices. `solve` adds noise to each matrix, multiplies them, records the mat-mul transcript, hashes the transcript, denoises the mat-mul, and finally returns both the hash and denoised mat-mul. `verify` does the same exact computation as the solver and confirm that it matches both the hash and denoised mat-mul of `solve`. Here `solve` and `verify` are both computationally expensive even though we need `verify` to be computationally cheap. The paper authors note that we can use SNARKs to speed up `verify` (remark 2.2) but the focus of this paper is on the usefulness of `solve`. This seems like a big detail that they skip over...\n",
    "\n",
    "You might say: if we are simply adding and removing noise to the mat-mul, what's the point? Can't an malicious solver simply skip the step of adding noise? No because they would have a different hashed mat-mul; remember we hash the noisy mat-mul and only remove the noise afterwards. Plus, even if the malicious solver would skip adding the noise, they would still need to multiply matricies and do the same computationally expensive work. So what's the point of the noise? The noise ensures that every time we run a mat-mul on two fixed matrices $A,B$, the result is different since we adding different random noise via a different random seed. This way, a malicious solver cannot precompute the result of $AB$, store it, and then return it anytime we multiply $A,B$ much faster than an honest solver.\n",
    "\n",
    "Let's take a closer look at the PoUW algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab3973d",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/solve_verify_alg.png\" alt=\"logo\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c982e2",
   "metadata": {},
   "source": [
    "In `solve` we take in a random seed `sigma`, and two nxn matrices `A,B` defined over the field $\\mathbb{F}_q$. We add noise to `A,B` in `encode` and we remove that noise in`decode`. The `matmul` step outputs `C`, the result of multiplying the two noisy matrices, and `z`, the `(n/r)^3` intermediate `rxr`block matrices used when computing the matmul known as the matmul transcript. Although it is not specified here, `z` is really the *hash* of all the `(n/r)^3` intermediate matrix blocks of the transcript."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce6bcc",
   "metadata": {},
   "source": [
    "Let's take a closer look at the `encode` algorithm.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/encode_alg.png\" alt=\"encode\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "`encode` takes in a random seed `sigma` and the `nxn` matrices `A,B`. \n",
    "\n",
    "* Step 1: from the seed, we generate four uniformly pseduorandom matrices `E_L, E_R, F_L, F_R`. The algorithm uses the word \"parse\" but you can think of this as using a random oracle `O` to deterministically generate `E_L, E_R, F_L, F_R` from the seed `sigma`. This means that the same seed `sigma` produces the same matrices every time but if we don't know sigma, the matrices appear uniformly random. (This is why this algorithm is pseudorandom instead of plain old random.) Mathematically for `E_L`, $P[E_L[i, j] == x] = 1/(2q+1)$ but $P[E_L[i, j] == x | \\sigma] = 1 \\text{ if } E_L[i, j] == x \\text{ else } 0$.\n",
    "* Step 2: we then multiply the `nxr` and `rxn` low-rank matrices to produce a full-rank `nxn` matrix of uniformly pseudorandom noise. We do this to construct both `E, F`, matrices of pure noise.\n",
    "* Step 3: Finally, we noise `A,B` by adding our purse noise matrices `E,F` to them.\n",
    "\n",
    "What is the time complexity? \n",
    "* Step 1 takes time $O(nr)$ because we are generating 4 matrices each with a total of `nr` elements. \n",
    "* Step 2 takes $cdot O(n^2r)$ because we multiply an `nxr` by `rxn` matrix twice.\n",
    "* Step 3 takes $O(n^2)$ time because we add two `nxn` matrices twice.\n",
    "* This yields a total complexity of $O(n^2 r)$ which is bottlenecked by running matmul twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e90834",
   "metadata": {},
   "source": [
    "Let's take a closer look at the `decode` algorithm.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/decode_alg.png\" alt=\"decode\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "`decode` takes in a random seed `sigma` and the denoised matmul result `C'`. \n",
    "\n",
    "* Step 1: Using the seed `sigma`, we can again get the same uniformly pseudorandom matrices we used in `encode`, `E_L, E_R, F_L, F_R`.\n",
    "* Step 2: We create `C''`, the inverse of `C'`.\n",
    "* Step 3: We subtract `C''` from `C'` to remove the noise `E,F` we originally added to `A,B`. This should leave us with the matmul result that has no noise whatsoever.\n",
    "\n",
    "What is the time complexity?\n",
    "* Step 1 takes time $O(nr)$ because we are generating 4 matrices each with a total of `nr` elements. \n",
    "* Step 2 takes time $O(n^2)$ because we multiply `nxr` and `rxn` matrices. In total we perform five mat-muls here. But we are careful to only multiply `nxr` and `rxn` matrices, never `nxn` matrices.\n",
    "* Step 3 takes `O(n^2)` for subtracting two `nxn` matrices from each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f71f5b",
   "metadata": {},
   "source": [
    "Let's take a closer look at the `MatMul_r` algorithm.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"assets/matmul_alg.png\" alt=\"MatMul algorithm\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "### Steps\n",
    "\n",
    "`MatMul_r` takes in an `n×k` matrix `A`, a `k×m` matrix `B`, and a block-size parameter `r`, and outputs an `n×m` matrix `C`. For simplicity, we assume our matrices are all square with `k=m=n`.\n",
    "\n",
    "**Step 1:** Initialize the `n×m` output matrix `C^(0)` as all zeros.\n",
    "\n",
    "**Step 2:** Partition `A,B,C` into `rxr` blocks. This creates a grid of `(n/r) × (k/r)` blocks for `A`, `(k/r) × (m/r)` blocks for `B`, and `(n/r) × (m/r)` blocks for `C`.\n",
    "\n",
    "**Step 3:** Compute the matrix product `C = A·B` using incremental block-wise multiplication $C_{i,j}^{(\\ell)} = C_{i,j}^{(\\ell-1)} + A_{i,\\ell} \\cdot B_{\\ell,j}$.\n",
    "\n",
    "**Step 4:** Output the final result.\n",
    "\n",
    "### Math\n",
    "\n",
    "Let's dive into the math a bit. The key equation is:\n",
    "\n",
    "$$\n",
    "C_{i,j}^{(\\ell)} = C_{i,j}^{(\\ell-1)} + A_{i,\\ell} \\cdot B_{\\ell,j}\n",
    "$$\n",
    "\n",
    "for $i \\in [n/r], j \\in [m/r], \\ell \\in [k/r]$, where $C_{i,j}^{(\\ell)}$ represents the `(i,j)` block of `C` after `ℓ` iterations.\n",
    "\n",
    "**Normal matrix multiplication** (entry-by-entry):\n",
    "- Indices `i, j, k` refer to individual matrix entries\n",
    "- Each output entry: $C[i,j] = \\sum_{k=1}^{n} A[i,k] \\cdot B[k,j]$\n",
    "\n",
    "**Block matrix multiplication** (block-by-block):\n",
    "- Indices `i, j, ℓ` refer to `r×r` **blocks**, not individual entries\n",
    "- Each output block computed incrementally:\n",
    "  - $C_{i,j}^{(1)} = A_{i,1} \\cdot B_{1,j}$ After iteration `ℓ=1` we've accumulated contributions from the first `r` terms of the dot product for all entries in the `i,j`-th block matrix\n",
    "  - $C_{i,j}^{(2)} = C_{i,j}^{(1)} + A_{i,2} \\cdot B_{2,j}$ After iteration `ℓ=2` we've accumulated contributions from the first `2r` terms of the dot product for all entries in the `i,j`-th block matrix\n",
    "  - $C_{i,j}^{(3)} = C_{i,j}^{(2)} + A_{i,3} \\cdot B_{3,j}$ After iteration `ℓ=3` we've accumulated contributions from the first `3r` terms of the dot product for all entries in the `i,j`-th block matrix\n",
    "  - ⋮\n",
    "  - $C_{i,j}^{(n/r)} = C_{i,j}^{(n/r-1)} + A_{i,n/r} \\cdot B_{n/r,j}$ After iteration `ℓ=n/r` we've accumulated contributions from all the `n` terms of the dot product for all entries in the `i,j`-th block matrix.\n",
    "\n",
    "where $A_{i,\\ell}$, $B_{\\ell,j}$, and $C_{i,j}^{(\\ell)}$ are all `r×r` matrices.\n",
    "\n",
    "### Complexity \n",
    "\n",
    "What is the time complexity?\n",
    "* Step 1 takes $O(nm)$ because `C` is an `nxm` matrix.\n",
    "* Step 2 is more of a conceptual step and requires no computation as we can just change our indexing strategy to divide `A,B,C` into `rxr` blocks.\n",
    "* Step 3 iterates over `n/r` row-block positions (index `i`), `m/r` column-block positions (index `j`), and `k/r` iteration steps (index `ℓ`) for a total of `(n/r) × (m/r) × (k/r) = (n/r)³` iterations for square matrices. Each iteration computes $C_{i,j}^{(\\ell)} = C_{i,j}^{(\\ell-1)} + A_{i,\\ell} \\cdot B_{\\ell,j}$ which costs $O(r^3)$ time using the naive matmul algorithm to multiply two `rxr` matrices $A_{i,\\ell}, B_{\\ell,j}$ plus the $O(r^2)$ cost of adding two `rxr` matrices. The total runtime is therefore ($(n/r)^3$ iterations) x ($O(r^3)$ cost per iteration) = $O(n^3)$.\n",
    "* Step 4 is $O(1)$ since it likely returns a pointer to the resultant matrix `C`.\n",
    "* In total, this algorithm costs $O(n^3)$ time.\n",
    "\n",
    "### Algorithm Corrections\n",
    "\n",
    "This algorithm is actually written imprecisely. First, it says we return $C^{(n)}$ but really the last iteration of $C$ occurs when $\\ell = n/r$, $\\ell$ does not even reach $n$ so we really return $C^{(n/r)}$. Second, we really want to return the hashed transcript of the matmul, not just the final output of the matmul.\n",
    "\n",
    "### Hashing\n",
    "\n",
    "Remark 2.1 states that instead of storing the entire transcript in order to hash it later, we can hash each of the (n/r)³ intermediate matrices as we encounter them and store just the hash, not the transcript itself.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126fd62",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894bd1d",
   "metadata": {},
   "source": [
    "Let's code up the algorithms `encode`, `decode`, `matmul`, `solve`, and `verify`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c699f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f50032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _low_rank_noise(n, m, q, r, sigma):\n",
    "    rng = np.random.default_rng(sigma)\n",
    "    El = rng.uniform(-q, q, (n, r))\n",
    "    Er = rng.uniform(-q, q, (r, m))\n",
    "    Fl = rng.uniform(-q, q, (n, r))\n",
    "    Fr = rng.uniform(-q, q, (r, m))\n",
    "    return El, Er, Fl, Fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f16db125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sigma, A, B, r, q):\n",
    "    # Step 1\n",
    "    n, m = A.shape[0], B.shape[1]\n",
    "    El, Er, Fl, Fr = _low_rank_noise(n, m,  q, r, sigma)\n",
    "\n",
    "    # Step 2\n",
    "    E = El @ Er\n",
    "    F = Fl @ Fr\n",
    "\n",
    "    # Step 3\n",
    "    A_noise = A + E\n",
    "    B_noise = B + F\n",
    "\n",
    "    return A_noise, B_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badf76e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sigma, A, B, C_noise, r, q):\n",
    "\n",
    "    # Step 1\n",
    "    n, m = A.shape[0], B.shape[1]\n",
    "    El, Er, Fl, Fr = _low_rank_noise(n, m, q, r, sigma)\n",
    "\n",
    "    # Step 2\n",
    "    C_noise_inverse = A @ Fl @ Fr + El @ Er @ (B + Fl @ Fr)\n",
    "\n",
    "    # Step 3\n",
    "    C = C_noise - C_noise_inverse\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f277df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(A, B, r):\n",
    "\n",
    "    # Step 1\n",
    "    n, k, m = A.shape[0], A.shape[1], B.shape[1]\n",
    "    C = np.zeros((n, m)) # this is really noisy C\n",
    "    hashes = {}\n",
    "\n",
    "    for l in range(1, n // r + 1):\n",
    "        for i in range(n // r): # ith block row\n",
    "            for j in range(m // r): # jth block column\n",
    "\n",
    "                # Step 2\n",
    "                A_block = A[i*r:(i+1)*r, l*r:(l+1)*r]\n",
    "                B_block = B[l*r:(l+1)*r, j*r:(j+1)*r]\n",
    "                C_block_prev = C[i*r:(i+1)*r, j*r:(j+1)*r]\n",
    "\n",
    "                # Step 3\n",
    "                C_block = C_block_prev + A_block @ B_block\n",
    "                C[i*r:(i+1)*r, j*r:(j+1)*r] = C_block\n",
    "\n",
    "                # Hash the intermediate l-th block state\n",
    "                hashes[(i, j, l)] = hash(C_block.tobytes())\n",
    "\n",
    "    # Step 4\n",
    "    return C, hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eaad1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(sigma, A, B, r, q):\n",
    "    A_noise, B_noise = encode(sigma, A, B, r, q)\n",
    "    C_noise, hashes = matmul(A_noise, B_noise, r)\n",
    "    C = decode(sigma, A, B, C_noise, r, q)\n",
    "    proof = (A, B, hashes)\n",
    "    return C, proof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca070a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(sigma, proof, r, q):\n",
    "    A, B, solver_hashes = proof\n",
    "\n",
    "    # the underscore means the verifier computes this, not the solver\n",
    "    A_noise, B_noise = encode(sigma, A, B, r, q)\n",
    "    _, verifier_hashes = matmul(A_noise, B_noise, r)\n",
    "\n",
    "    return solver_hashes == verifier_hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711f4f6",
   "metadata": {},
   "source": [
    "Let's confirm this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d0b36aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11],\n",
       "       [12, 13, 14, 15]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 4\n",
    "r = 1\n",
    "q = 2\n",
    "sigma = 42\n",
    "\n",
    "# 16x16 integer matrices\n",
    "A = np.arange(n*n).reshape(n, n)\n",
    "B = np.arange(n*n).reshape(n, n)\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a0a98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.09582419],\n",
       "       [-0.24448624],\n",
       "       [ 1.43439168],\n",
       "       [ 0.78947212]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "El, Er, Fl, Fr = _low_rank_noise(n, n, q, r, sigma)\n",
    "El"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ec21ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 54.47726996,  60.362594  ,  72.1566106 ,  82.22354554],\n",
       "       [155.76382652, 178.04727824, 185.72584711, 197.67335112],\n",
       "       [252.85499405, 291.220621  , 310.74727677, 335.78047078],\n",
       "       [353.17525275, 407.86623602, 426.95422569, 456.44879456]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C, proof = solve(sigma, A, B, r, q)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9150329a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1) 959115215560911177\n",
      "(0, 1, 1) -6345059038964853663\n",
      "(0, 2, 1) 3086096880040650956\n",
      "(0, 3, 1) 8986767763871355705\n",
      "(1, 0, 1) -3973625406403518917\n",
      "(1, 1, 1) -103071798878682043\n"
     ]
    }
   ],
   "source": [
    "hashes = proof[-1]\n",
    "for i, (k, v) in enumerate(hashes.items()):\n",
    "    print(k, v)\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3dbf796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification = verify(sigma, proof, r, q)\n",
    "verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864cd267",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. So we can technically use any projection matrix to add noise, right? They all satisfy the property that AA^T=I such that the entire computation has no noise but the intermediate computation does have noise added because we have not yet summed up all the terms of the dot product in the transcript, right? But the advantage of using specifically the Hadamard transform as our projection matrix is that it allows us to compute the matmul faster in $O(n^2 \\log n)$ time via FFT instead of $O(n^2)$ time, right? Theoretically, it is faster to perform Hadamard matmul with FFT but in practice is it faster? To clarify, when we multiply with the Hadamard matrix, it just means that we can add noise much faster than a regular mat mul but of course we still must compute the regular matmul in time O(n^3), there is no speedup for this.\n",
    "2. Most LLM inference kernels are memory bound, not compute bound. What's the best tiling parameter `r` for the hardware? Does it align with tensor cores? If we now are computing hashes, does the make us more compute bound or less?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a18cba",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "useful-work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
